---
title: "The Practices of Machine Learning"
author: "Andrew R. Bower"
date: "`r format(Sys.Date())`"
output:
  html_document:
    theme: readable
    toc: true
    toc_depth: 2
    toc_float: true
    code_download: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, error = FALSE, warning = FALSE, comment = NA)
```

```{r, message = FALSE}
if (!require("pacman")) install.packages("pacman"); library(pacman)

pacman::p_load(tidyverse, broom, janitor, here, rmarkdown, rstanarm, haven, sf, ggthemes, tufte, Hmisc, modelr, DescTools, gridExtra)
```


# Key Terminology

* ***Classification***. This can refer to using models to place observations into separate groups (cluster) or the modeling of a categorical, most often, binary outcome. 
* ***Regression***. In the simplest case, this refers to the modeling of a continuous outcome, which in parametric models, involves using the Gaussian distribution. 
* ***Imbalance***. This occurs when the outcome of interest has a smaller proportion of one class, which is also referred to as a skewed outcome. This most often occurs with clinical outcomes that have a low proportion of positive cases in the population, such as suicide attempts or cancer diagnosis. 
* ***Resampling***. This refers to the general process of selecting observations (rows) from a dataset, oftentimes repeated. In the simplest case, this involves splitting a dataset into two partitions, while more complicated methods repeat this process a large number of times, and may reuse observations from for populating the newly partitioned datasets (bootstrap sampling). This is done so that the data used to train the model is not also used to produce fit metrics, thus driving less biased assessments of prediction performance.
* ***Predict probability***. When the outcome is binary, most algorithms generate probabilities of belonging to the positive class (coded 1, as opposed to 0). This vector of probabilities (one for each observation) can be used to assess fit (using area under the receiver operating characteristic curve (AUC) or area under the precision recall curve (AUPRC), or to generate predicted class labels based on a cutoff, and then further assessed with a host of fit metrics (i.e., accuracy, recall). 

# 3.2 Comparing Alorithms and Models

In machine learning there is generally no reason to have an a priori notion that one method will fit better than any other: the _no free lunch_ theorem (Wolpert, 1996). As such, you should run multiple algorithms and a regression, tried and true, or with selection (i.e., lasso) and compare fits. Conclusions often can get more support if buoyed by multiple algorithms; epistemologically, this also aligns with the idea of _consilience_ where multiple different lens of understanding that explain a phenomenon or the mechanisms that generate a phenomenon similarly is a sign of a strong understanding. 

# 3.3 Model Fit

To motivate this section let's consider a linear regression model on a continuous outcome and predictor. To evaluate how well our linear algorithm fits the data, we can calculate a _Mean Square Error_ (MSE): 

$\operatorname{MSE} = \frac{1}{n} \sum\limits_{i=1}^{N} (y_i - \hat{f}(x_i))^2$.

Where the misfit as measured by MSE is small if the predicted and actual outcomes for observation _i_ are similar.

We could also calculate $\operatorname{R}^2$, which gives us the proportion of variance explained in the outcome as predicted by the independent variable(s). 

We can calculate $\operatorname{R}^2$ in the ANOVA way, and the regression way. In the ANOVA way: 

$\operatorname{SS_{reg}} = \sum\limits_{i}(\hat{f}(x_i) - \bar{y})^2$, the sum of squares of regressions

$\operatorname{SS_{tot}} = \sum\limits_{i}(y_i - \bar{y})^2$, the total sum of squares where $\bar{y}$ is the mean outcome of observed data

$\operatorname{R}^2 = \frac{\operatorname{SS_{reg}}}{\operatorname{SS_{tot}}}$ will give you the $\operatorname{R}^2$.

You can also calculate the $\operatorname{R}^2$ as the squared correlation between the vector of predicted responses and actual responses. This is often best represented visually in a plot. They don't seem to provide access to the data to produce the figure to do this. 

They regress PKU exposure in mothers on infant cognitive development as measured by IQ. The model fit was linear and showed $I\hat{Q}_i = 107.43 - 2.91 * PHE_i$. The model had an MSE of 174.487 and an $R^2$ of 0.699. It does seem that 70% of the variance explained is good, but you can see from the plot (in the book) that the relationship is polynomial and they could test wit polynomial regression from $PHE^2 ... PHE^{20}$. The other option is to run a random forest model with minimal trees. They discuss fitting larger trees that fit the data better, but the larger trees maximize on small nuances and occurrences within the data thus fitting the data too well. Many of these fit statistics should not be thought of as all-or-nothing, but rather to a degree. Since _overfititng_ pertains to the degree of generalizability from our fit model. We don't want to be so good at describing our sample that we loose the forest for all the trees.

```{r}
# Read Data into R
dd <- read_table(here("data-raw/apexpos.dat"))

# Set names
names(dd) <- c('id', 'apexpos', 'fsiq7')

# Look at the data
glimpse(dd)

## Plotting Full Scale IQ by PHE
dd |> 
  ggplot(aes(x = apexpos, y = fsiq7)) +
  geom_point(col = "steelblue2", alpha = 1/3) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x, color = "coral3", linewidth = 1) +
  theme_classic() +
  labs(x = "Phenylalanine Exposure (PHE)",
       y = "Full Scale IQ at Age 7") 


# Model
fit <- with(dd, lm(fsiq7 ~ apexpos))

RSS <- c(crossprod(fit$residuals))
MSE <- RSS / length(fit$residuals)
print(MSE)

RMSE <- sqrt(MSE)
print(RMSE)

RMSE2 <- RSS / fit$df.residual
print(RMSE2)
```

For fun, here is what fitting a polynomial would look like. Hard to follow the code provided. 

```{r}
dd |> 
  mutate(apexpos2 = apexpos^2) -> dd2

# Model
fit2 <- with(dd2, lm(fsiq7 ~ apexpos + apexpos2))
summary(fit2)

RSS <- c(crossprod(fit2$residuals))
MSE <- RSS / length(fit2$residuals)
print(MSE)

RMSE <- sqrt(MSE)
print(RMSE)

RMSE2 <- RSS / fit2$df.residual
print(RMSE2)

## Plotting Full Scale IQ by PHE poly
dd |> 
  ggplot(aes(x = apexpos, y = fsiq7)) +
  geom_point(col = "steelblue2", alpha = 1/3) +
  geom_smooth(method = "lm", se = FALSE, 
              formula = y ~ x + I(x^2), color = "coral3", linewidth = 1) +
  theme_classic() +
  labs(x = "Phenylalanine Exposure (PHE)^2",
       y = "Full Scale IQ at Age 7") 

```

We can also fit a random forest model that follows the data more closely. 


```{r}
# Load necessary packages
pacman::p_load(randomForest, rpart, rpart.plot)

# Take a look at your data
dd

# Fit a Random Forest model
set.seed(1983)
rf_model <- randomForest(fsiq7 ~ apexpos, data = dd, ntree = 100, importance = TRUE)

# Predict using the Random Forest model
dd$predicted_fsiq7 <- predict(rf_model, dd)

# Look at the predictions
head(dd)

# Aggregate the data to fewer steps
# Here we are manually setting up the groups for demonstration purposes
dd_grouped <- dd |> 
  mutate(group = ntile(apexpos, 4)) |>   # Create 4 groups based on apexpos
  group_by(group) |> 
  dplyr::summarize(apexpos = mean(apexpos), fsiq7 = mean(fsiq7), predicted_fsiq7 = mean(predicted_fsiq7))


# Print the model summary
print(rf_model)

# Plot variable importance
importance(rf_model)
varImpPlot(rf_model)

# Extract a single tree from the Random Forest
single_tree <- getTree(rf_model, k = 1, labelVar = TRUE)

# Fit a similar tree using rpart for visualization
rpart_model <- rpart(fsiq7 ~ apexpos, data = dd, method = "anova", control = rpart.control(cp = 0.02))

# Visualize the tree
rpart.plot(rpart_model)
```

Let's visiualize this model

```{r}
# Plot using ggplot2
dd |> 
  ggplot(aes(x = apexpos, y = fsiq7)) +
  geom_point(col = "steelblue2", alpha = 1/3) +
  geom_step(data = dd_grouped, aes(x = apexpos, y = predicted_fsiq7), color = 'coral3') +
  theme_classic() +
  labs(title = "Random Forest",
       x = "Phenylalanine Exposure (PHE)",
       y = "Full Scale IQ at Age 7") 
```

# 3.4 Bias-Variance Trade-Off

Every dataset has a finite amount of information that may be gleaned or extracted. The amount of information is related to many factors, most critically sample size and the quality of the predictors. What follows then is that the more information in a dataset the more information can be learned, in theory; but, generally generates more complex models. Model complexity can be broken down and evaluated as a tradeoff between: error, bias, and variance. 

Error, or Variance of Error, is often called _irreducible error_ or the stuff we cannot control with our models. Basically, how the same scores on X can lead to different Y. A prediction model cannot differentiate these cases. What we do have control over (somewhat) is _bias_, or whether our estimates on average across many random draw from the population equal to the true values in the population. _Variance_ is the variability or precision of these estimates. Ideally we want low bias and low variance but often hard to control

When a model is biased is when we fit a linear model to data that are not linear. That prediction is biased because it is not flexible enough to model the underlying relationships. So, we can test more complex relationships, varying polynomial parameters into the model. When we vary a parameter this is called _tuning parameters_ also known as a hyperparameter (tuning too hard on the data also increases the bias). 

## Tuning Parameters

Tuning parameters refer to the "settings" of each algorithm that control the behavior (flexibility) of each algorithm. When we have polynomial regressions adjusting the polynomial is the tuning parameter of the model. We can see how this works here with an over-tuned model:

```{r}
# Fit a hyper tuned polynomial
fit1 <- lm(fsiq7 ~ poly(apexpos, 20), data = dd2)
summary(fit1)

# Plot it
# Fit polynomial regression models and predict
poly_predictions <- list()
degrees <- 1:20

for (degree in degrees) {
  formula <- as.formula(paste("fsiq7 ~ poly(apexpos, ", degree, ")", sep = ""))
  fit <- lm(formula, data = dd2)
  pred <- predict(fit, newdata = dd2)
  poly_predictions[[degree + 1]] <- data.frame(apexpos = dd$apexpos, fsiq7 = pred, degree = degree)
}

# Combine all predictions into one data frame
all_predictions <- bind_rows(poly_predictions)

# Plot using ggplot2
ggplot(dd, aes(x = apexpos, y = fsiq7)) +
  geom_point(col = "steelblue2", alpha = 1/3) +
  geom_line(data = all_predictions, aes(y = fsiq7, color = as.factor(degree)), show.legend = FALSE) +
  labs(title = 'Polynomial Regression Lines from Degree 1 to 20',
       x = 'Phenylalanine Exposure (PHE)',
       y = 'Full Scale IQ at Age 7') +
  theme_classic() +
  scale_color_viridis_d()
```

We can show how the bias variance tradeoff works with the above by splitting our data into a test and training set. We can assess variance by how much the parameter estimates vary across 100 subsamples, which indicates how much the underlying functional form varies. We lift the code they provide, which is really hard to follow, and try and fix it. The idea here is to look at MSE for a poorly fit, and overly fit (i.e., not flexible and too flexible) algorithm. 


```{r}
# Trying to recreate Figure 3.7 from their code provide - again, the code is terribly annotated and they recommend seeking ChatGPT for any clarity - which is awful. Why would people pay for this to just complete it with ChatGPT?

# Create a split into training and testing sets
set.seed(1983)  # For reproducibility
dd <- dd |>  
  arrange(apexpos)

train_indices <- sample(seq_len(nrow(dd)), size = 0.8 * nrow(dd))
data_train <- dd[train_indices,]
data_test <- dd[-train_indices,]

# Function to calculate MSE for a given sample and polynomial degree
calculate_mse <- function(train_data, test_data, degree) {
  lm_out <- lm(fsiq7 ~ poly(apexpos, degree), data = train_data)
  mse_train <- mean((train_data$fsiq7 - predict(lm_out))^2)
  mse_test <- mean((test_data$fsiq7 - predict(lm_out, newdata = test_data))^2)
  list(train = mse_train, test = mse_test)
}

# Generate MSE values for 1000 iterations and polynomial degrees 1 to 8 given their figure
set.seed(1983)  # For reproducibility
results <- map_dfr(1:1000, function(i) {
  samp1 <- data_train %>% sample_n(50)
  map_dfr(1:8, function(j) {
    mse <- calculate_mse(samp1, data_test, j)
    tibble(iteration = i, degree = j, mse_train = mse$train, mse_test = mse$test)
  })
})

# Winsorize the MSE values to handle extreme values
results <- results |> 
  mutate(mse_train = Winsorize(mse_train, probs = c(0.10, 0.90)),
         mse_test = Winsorize(mse_test, probs = c(0.10, 0.90)))

# Calculate mean and variance of MSE values
summary_results <- results |> 
  group_by(degree) |> 
  summarise(mean_train = mean(mse_train), mean_test = mean(mse_test),
            var_train = var(mse_train), var_test = var(mse_test))

# Take a look at summary_results
summary_results
```

```{r}
# Prepare data for plotting
mean_data <- summary_results |> 
  select(degree, mean_train, mean_test) |> 
  pivot_longer(cols = c(mean_train, mean_test), names_to = "Data", values_to = "mse")

var_data <- summary_results |> 
  select(degree, var_train, var_test) |> 
  pivot_longer(cols = c(var_train, var_test), names_to = "Data", values_to = "mse")

# Plot Mean MSE
p1 <- ggplot(mean_data, aes(x = degree, y = mse, color = Data)) +
  geom_line() +
  xlab('Power') +
  ylab('Mean MSE') +
  theme_classic() +
  scale_colour_grey(start = 0, end = 0.7) +
  ggtitle("Mean MSE by Polynomial Degrees")

# Plot MSE Variance
p2 <- ggplot(var_data, aes(x = degree, y = mse, color = Data)) +
  geom_line() +
  xlab('Power') +
  ylab('MSE Variance') +
  theme_classic() +
  scale_colour_grey(start = 0, end = 0.7) +
  ggtitle("MSE Variance by Polynomial Degrees")

# Print plots side by side
grid.arrange(p1, p2, ncol = 2)
```

But, what we can learn from this is: 

1. As power increases, the training MSE monotonically decreases.
2. Simultaneously, as the training MSE decreases, the testing mean and variance increase. 
3. We want to decrease bias and variance. High degrees of variance are especially problematic with small sample sizes which requires the use of flexible algorithms. 
4. "How well will my model fit on a holdout sample?" is the question we want to answer because we won't be able to visualize like we did here since most of our projects will have more than one predictor. 

# 3.5 Resampling

To answer the question "How well will my model fit on a holdout sample?" there were a number of methods developed to provide answers without the need for a second dataset. 

* _Model Assessment_: In estimating a single model, or after choosing a final model, determining the most unbiased assessment of model fit, either on new data or what it would be on new data (e.g., AIC, BIC, QAIC, WAIC, LOOCV, etc.).
* _Model Selection_: Estimating the performance of multiple algorithms/models and choosing a final model among these. 

The _train-validate-test_ paradigm that splits a sample into 50% train, 25% validate, and 25% test to evaluate the models' performance on an out of sample (test) data to see how close our predictive accuracy is; but, this requires a sample size sufficiently large (e.g., 5000), which we often do not have access.

## 3.5.1 k-Fold CV

If you have a smaller sample size -- often the case-- then you can keep your sample in tact. In k-fold you can split the data into partitions a number of partitions. Here, you do not rerun on the holdout partitions instead _the model is kept fixed and used to create predicted responses for the observations_. Using the MSE as an example, this formally manifests itself as:

$\operatorname{MSE_{holdout}} = \frac{1}{n} \sum\limits_{i=1}^{k}(y_i, holdout - \hat{f}training(x_i, holdout))$

Which allows us to compute the k-fold MSE:

$\operatorname{MSE_{k-fold}} = \frac{1}{n} \sum\limits_{i=1}^{k}(\operatorname{MSE_k})$

We can do this 10x's, because increasing the partitioning can reduce the chance for aberrant result due to chance, but as we increase it increases computing time. 

## 3.5.2 Nested CV

Nested CV works by creating two sets of loops, one loop for model selections, and one loop for model assessment, and can be seen as a repeated extension of the training-validation-test paradigm. Here is an example from their book:

```{r}
# Simulation and Model Selection with Highest R^2:
# Load necessary libraries
pacman::p_load(caret, AppliedPredictiveModeling, psych)

# Set seed for reproducibility
set.seed(1983)

# Generate latent variables
lat.var1 <- rnorm(30000)
lat.var2 <- rnorm(30000)

# Generate response variable
y <- 0.5 * lat.var1 + 0.5 * lat.var2 + rnorm(30000, 0, 1)

# Summarize the linear model
summary(lm(y ~ lat.var1 + lat.var2))

# Create a data frame with the variables
dat1 <- data.frame(y = y, x1 = lat.var1, x2 = lat.var2)

# Initialize a matrix to store R^2 values
ret <- matrix(NA, 200, 2)

# Loop for 200 repetitions
for (i in 1:200) {
  # Sample data for training models
  samp1 <- dat1[sample(1:30000, 100),]
  samp2 <- dat1[sample(1:30000, 100),]

  # Train linear models with cross-validation
  lm1 <- train(y ~ ., data = samp1, method = "lm", trControl = trainControl(method = "cv"))
  ret[i, 1] <- lm1$results$Rsquared

  lm2 <- train(y ~ ., data = samp2, method = "lm", trControl = trainControl(method = "cv"))
  ret[i, 2] <- lm2$results$Rsquared
}

# Describe the R^2 values
psych::describe(ret)

# Compute the mean of the highest R^2 values across repetitions
rowM <- apply(ret, 1, function(x) max(x, na.rm = TRUE))
mean(rowM)
```

Now, we can do what they did for demonstration showing the value of Nested CV

```{r}
# Initialize a vector to store R^2 values for nested CV
ret2 <- rep(NA, 200)

# Loop for 200 repetitions
for (i in 1:200) {
  # Sample data for nested CV
  samp1 <- dat1[sample(1:30000, 100),]
  samp2 <- dat1[sample(1:30000, 100),]

  # Create folds for cross-validation
  folds.samp1 <- createFolds(samp1$y, k = 5, list = FALSE)
  folds.samp2 <- createFolds(samp2$y, k = 5, list = FALSE)

  # Initialize a vector to store R^2 values for each fold
  rsq <- rep(NA, 5)

  # Nested CV: inner loop
  for (j in 1:5) {
    lm1 <- train(y ~ ., data = samp1[folds.samp1 != j,], method = "lm", trControl = trainControl(method = "cv"))
    lm2 <- train(y ~ ., data = samp2[folds.samp2 != j,], method = "lm", trControl = trainControl(method = "cv"))

    # Select the model with the highest R^2 and evaluate on the holdout fold
    if (lm1$results$Rsquared > lm2$results$Rsquared) {
      rsq[j] <- cor(samp1[folds.samp1 == j, "y"], predict(lm1, samp1[folds.samp1 == j,]))^2
    } else {
      rsq[j] <- cor(samp2[folds.samp2 == j, "y"], predict(lm2, samp2[folds.samp2 == j,]))^2
    }
  }

  # Store the mean R^2 value for the current repetition
  ret2[i] <- mean(rsq)
}

# Summarize the results of nested CV
summary(ret2)
```

In the additional resources they provide a demo comparing two trained models: a linear model (lm1) and a MARS model (rf1) using the earth method from caret. MARS (Multivariate Adaptive Regression Splines) is a type of regression model used for predicting numerical outcomes. It is a flexible model that can capture non-linear relationships between the predictors (independent variables) and the response variable (dependent variable). The MARS model automatically selects and combines piecewise linear segments to best fit the data. The utility of this example for us is simply showing the value of Nested CV in selecting between different types of regression algorithms. 


```{r}
# Set seed for reproducibility
set.seed(1983)

# Generate latent variables
lat.var1 <- rnorm(1000)
lat.var2 <- rnorm(1000)

# Generate response variable
y <- 0.5 * lat.var1 + 0.5 * lat.var2 + rnorm(1000, 0, 1)

# Create a data frame with the variables
dat1 <- data.frame(y = y, x1 = lat.var1, x2 = lat.var2)

# 5-Fold Cross-Validation
folds.samp1 <- createFolds(dat1$y, k = 5, list = FALSE)

rsq <- rep(NA, 5)
method <- rep(NA, 5)

for (j in 1:5) {
  # Train linear model
  lm1 <- train(y ~ ., data = dat1[folds.samp1 != j, ], method = "lm", trControl = trainControl(method = "cv"))
  
  # Train MARS model
  rf1 <- train(y ~ ., data = dat1[folds.samp1 != j, ], method = "earth", trControl = trainControl(method = "cv"))
  
  # Select model with highest R^2 and evaluate on holdout fold
  if (lm1$results$Rsquared > max(rf1$results$Rsquared)) {
    rsq[j] <- cor(dat1[folds.samp1 == j, "y"], predict(lm1, dat1[folds.samp1 == j, ]))^2
    method[j] <- "lm"
  } else {
    rsq[j] <- cor(dat1[folds.samp1 == j, "y"], predict(rf1, dat1[folds.samp1 == j, ]))^2
    method[j] <- "rf"
  }
}

# Print the R^2 values and the chosen methods
print(rsq)
print(method)

# Summarize the R^2 values
summary(rsq)

# Visualize the R^2 values
rsq_data <- data.frame(Fold = 1:5, R_squared = rsq, Method = method)

# Print a summary of the methods chosen
table(method)

# Plot R^2 values
rsq_data |> 
ggplot(aes(x = factor(Fold), y = R_squared, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_classic() +
  labs(title = "R-squared Values for Each Fold",
       x = "Fold",
       y = "R-squared",
       fill = "Model") +
  scale_fill_manual(values = c("lm" = "steelblue3", "rf" = "coral3"))
```

## 3.5.3 Bootstrap Sampling

The main distinction between bootstrap and k-fold CV is that where k-fold CV uses sampling _without_ replacement, the bootstrap uses sampling _with_ replacement.This enables a larger number of repeats compared with k-fold CV, because it's less taxing to split the dataset. 

## 3.5.4 Recommendations

There is no free lunch - so each will have bias depending. 

# 3.6 Classification

The same recommendations and principles apply to classification of binary outcomes as the continuous outcomes above. Except with imbalanced or skewed binary outcomes. The most straightforward way of assessing predictions is the accuracy metric: $\text{Accuracy} = \frac{1}{n} \sum_{i=1}^n I(y_i = \hat{y}_i)$. The I here stands for "1" if it is predicted. This requires us to make a predicted class label for each observation. 
